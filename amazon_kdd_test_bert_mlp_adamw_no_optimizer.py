# -*- coding: utf-8 -*-
"""Amazon_KDD_Test_2layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RjJu5UyiZB6S8_83cIeGNBMTg-bZS9ob

# Import
"""
import os
import math
import numpy as np
import pandas as pd 
from tqdm.auto import tqdm
import tensorflow as tf
from transformers import BertTokenizer

"""# Load cleaned Dataset"""

test_df = pd.read_csv('Export/test_cleaned_query_title_brand.csv')
test_df = test_df.drop(test_df.columns[[1,2,3,4,5,6,7,8,9]], axis=1)
test_df.info()
result_df = pd.read_csv('Export/submission-v0.3.csv')
result_df.info()

"""# Load Model and Tokenizer"""

esci_model = tf.keras.models.load_model('esci_model_mlp_adamw_no_optimizer')

tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')

def prepare_data(text_a, text_b, tokenizer):
    token = tokenizer(
            text=text_a,
            text_pair=text_b,
            max_length=64, 
            truncation= 'only_second', 
            padding='max_length', 
            add_special_tokens=True,
            return_tensors='tf'
        )
    return {
        'input_ids': tf.cast(token.input_ids, tf.float64),
        'token_type_ids': tf.cast(token.token_type_ids, tf.float64),
        'attention_mask': tf.cast(token.attention_mask, tf.float64)
    }

def make_prediction(model, processed_data, classes=['exact', 'substitute', 'complement', 'irrelevant']):
    probs = model.predict(processed_data)[0]
    return classes[np.argmax(probs)]

esci_model.summary()

"""# Run prediction"""

for i in tqdm(range(90000,394367)):
  processed_data = prepare_data(test_df.iloc[i,1],test_df.iloc[i,2], tokenizer)
  result_df.iloc[i, 0] = test_df.iloc[i,0]
  result_df.iloc[i, 1] = make_prediction(esci_model, processed_data=processed_data)
  if(i%10000 == 0):
    result_df.to_csv('Export/submission-v0.3.csv', index=False)

result_df.head()

result_df.to_csv('Export/submission-v0.3.csv', index=False)